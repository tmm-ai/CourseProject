# -*- coding: utf-8 -*-
"""cs410_text_comp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zcMMw8xe6vh9rMPlBB_i-HZGrxsk5UJj

This will link your Google Drive account to Google Colabs. You will be given a link to click and from there you will provide permission to access google drive. Then, a verification code is provided to paste in the code below.
"""

#from google.colab import drive
#drive.mount('/content/drive')

pip install transformers

"""# New Section"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from transformers import TFRobertaForSequenceClassification, TFBertForSequenceClassification
from transformers import RobertaTokenizerFast, BertTokenizerFast
import tensorflow.keras.backend as K
from sklearn.model_selection import train_test_split

#read data
train = pd.read_json("data/train.jsonl", lines=True)
test =  pd.read_json("data/test.jsonl", lines=True)
train = train.sample(frac=1, random_state = 5).reset_index(drop=True) #shuffle

def prepare_context(text_lst): #prepare context for input
  res = ""
  for i in range(len(text_lst)):
    res += " # CONTEXT " + str(i)+ " # " + text_lst[i]
  return res

#prepare input
train["input"] = train["response"].apply(lambda x: " # RESPONSE # "+x) + train["context"].apply(prepare_context) #add response and context together
train["input"] = " ** TWEET ** " + train["input"] #(not really important, used this to differentiate amongst external data but not needed in the end)
train["label_numeric"] = train["label"].map({"SARCASM": 1, "NOT_SARCASM": 0})
train.head()

#train-test split
train_texts, val_texts, train_labels, val_labels = train_test_split(list(train["input"].values), list(train["label_numeric"].values), test_size=.2, random_state = 5)

#tokenize data (prepare inputs, attention masks, and special tokens)
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=64)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=64)
train_dataset = tf.data.Dataset.from_tensor_slices(( #creates a tensorflow dataset object that can be used to train
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))

#train model
K.clear_session() #initializes random parameters
model = TFRobertaForSequenceClassification.from_pretrained('roberta-large')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=4, batch_size=16, validation_data=val_dataset.shuffle(100).batch(16))

#predict on validation set
val_pred = []
for text in tqdm(val_texts): #predict each validation row with progress bar
    val_encodings = tokenizer.encode(text,
                             truncation=True,
                             padding=True,
                             max_length=128, #add in a little more context
                             return_tensors="tf")
    logits = model.predict(val_encodings)[0] #outputs logits
    val_pred.append(tf.nn.softmax(logits, axis=1).numpy()[0]) #converts logits to probabilities
val_pred_labels = np.argmax(val_pred, axis=-1) #outputs higher probable class
f1_score(val_labels, val_pred_labels)

#save model
model.save_pretrained("drive/MyDrive/data/roberta_model")

#load model and tokenizer
tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')
model = TFRobertaForSequenceClassification.from_pretrained("drive/MyDrive/data/roberta_model")

#predict on validation set
val_pred = []
for text in tqdm(val_texts): #predict each validation row with progress bar
    val_encodings = tokenizer.encode(text,
                             truncation=True,
                             padding=True,
                             max_length=128, #add in a little more context for predict
                             return_tensors="tf")
    logits = model.predict(val_encodings)[0] #outputs logits
    val_pred.append(tf.nn.softmax(logits, axis=1).numpy()[0]) #converts logits to probabilities
val_pred_labels = np.argmax(val_pred, axis=-1) #outputs higher probable class
f1_score(val_labels, val_pred_labels)

#prepare test input
test["input"] = test["response"].apply(lambda x: " # RESPONSE # "+x) + test["context"].apply(prepare_context)
test["input"] = " ** TWEET ** " + test["input"]
test_texts = list(test["input"].values)
test_pred = []

#tokenize and predict on test set
for text in tqdm(test_texts):
    test_encodings = tokenizer.encode(text,
                             truncation=True,
                             padding=True,
                             max_length=128, #add in a little more context for predict
                             return_tensors="tf")
    logits = model.predict(test_encodings)[0]
    test_pred.append(tf.nn.softmax(logits, axis=1).numpy()[0])
test_pred_labels = np.argmax(test_pred, axis=-1)

#write test output to answer.txt
with open('drive/MyDrive/data/answer2.txt', 'w') as the_file:
    for i in range(len(test)):
        if test_pred_labels[i] == 1:
            the_file.write(test.loc[i, "id"]+",SARCASM\n")
        else:
            the_file.write(test.loc[i, "id"]+",NOT_SARCASM\n")
